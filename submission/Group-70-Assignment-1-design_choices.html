
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Literature Review: Extractive and Abstractive Question Answering</title>
        <style>
            * {
                margin: 0;
                padding: 0;
                box-sizing: border-box;
            }
            
            body {
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                line-height: 1.7;
                color: #2c3e50;
                background-color: #ffffff;
            }
            
            .container {
                max-width: 900px;
                margin: 0 auto;
                padding: 40px 30px;
            }
            
            h1 {
                color: #1a5c9e;
                font-size: 28px;
                margin: 30px 0 20px 0;
                padding-bottom: 15px;
                border-bottom: 3px solid #1a5c9e;
                page-break-after: avoid;
            }
            
            h2 {
                color: #2980b9;
                font-size: 22px;
                margin: 25px 0 15px 0;
                padding-top: 10px;
                page-break-after: avoid;
            }
            
            h3 {
                color: #3498db;
                font-size: 18px;
                margin: 18px 0 12px 0;
                page-break-after: avoid;
            }
            
            h4 {
                color: #34495e;
                font-size: 15px;
                margin: 12px 0 8px 0;
                page-break-after: avoid;
            }
            
            p {
                margin: 12px 0;
                text-align: justify;
            }
            
            ul, ol {
                margin: 15px 0 15px 30px;
            }
            
            li {
                margin: 8px 0;
            }
            
            table {
                width: 100%;
                border-collapse: collapse;
                margin: 20px 0;
                page-break-inside: avoid;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }
            
            th {
                background-color: #2980b9;
                color: white;
                padding: 12px;
                text-align: left;
                font-weight: 600;
                border: 1px solid #2c3e50;
            }
            
            td {
                padding: 10px 12px;
                border: 1px solid #ecf0f1;
                background-color: #f9f9f9;
            }
            
            tr:nth-child(even) td {
                background-color: #f5f5f5;
            }
            
            tr:hover td {
                background-color: #f0f7ff;
            }
            
            code {
                background-color: #f4f4f4;
                padding: 2px 6px;
                border-radius: 3px;
                font-family: 'Courier New', monospace;
                color: #c7254e;
            }
            
            pre {
                background-color: #f4f4f4;
                border-left: 4px solid #2980b9;
                padding: 12px;
                margin: 15px 0;
                border-radius: 4px;
                overflow-x: auto;
                page-break-inside: avoid;
                font-family: 'Courier New', monospace;
                font-size: 12px;
            }
            
            blockquote {
                border-left: 4px solid #3498db;
                padding-left: 15px;
                margin: 15px 0;
                color: #555;
                font-style: italic;
            }
            
            .diagram {
                background-color: #ecf0f1;
                border: 1px solid #bdc3c7;
                border-radius: 4px;
                padding: 15px;
                margin: 20px 0;
                page-break-inside: avoid;
                font-family: 'Courier New', monospace;
                font-size: 11px;
                white-space: pre;
                overflow-x: auto;
            }
            
            .metrics {
                background-color: #e8f4f8;
                border-left: 4px solid #16a085;
                padding: 15px;
                margin: 15px 0;
                page-break-inside: avoid;
            }
            
            .highlight {
                background-color: #fffacd;
                padding: 2px 4px;
                border-radius: 2px;
            }
            
            strong {
                color: #2c3e50;
                font-weight: 600;
            }
            
            em {
                color: #34495e;
                font-style: italic;
            }
            
            hr {
                border: none;
                border-top: 2px solid #ecf0f1;
                margin: 30px 0;
                page-break-after: avoid;
            }
            
            .section {
                page-break-inside: avoid;
            }
            
            .key-result {
                background-color: #d5f4e6;
                border-left: 4px solid #27ae60;
                padding: 12px;
                margin: 10px 0;
                page-break-inside: avoid;
            }
            
            .warning {
                background-color: #fadbd8;
                border-left: 4px solid #e74c3c;
                padding: 12px;
                margin: 10px 0;
            }
            
            a {
                color: #2980b9;
                text-decoration: none;
            }
            
            a:hover {
                text-decoration: underline;
            }
            
            @media print {
                body {
                    margin: 0;
                    padding: 0;
                }
                .container {
                    padding: 20px;
                }
                h1, h2, h3 {
                    page-break-after: avoid;
                }
                table, pre, .diagram {
                    page-break-inside: avoid;
                }
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>Design Choices and Architecture</h1>

<h2>Team 70 - Contributors</h2>

<table>
<tr>
<th>Name</th>
<th>Email Address</th>
<th>Contributions %</th>
</tr>
<tr>
<td>NEERAJ BHATT</td>
<td>2024aa05020@wilp.bits-pilani.ac.in</td>
<td>100%</td>
</tr>
<tr>
<td>V. S. BALAKRISHNAN</td>
<td>2024aa05017@wilp.bits-pilani.ac.in</td>
<td>100%</td>
</tr>
<tr>
<td>KURUVELLA VENKATA SAI UPENDRA</td>
<td>2024aa05016@wilp.bits-pilani.ac.in</td>
<td>100%</td>
</tr>
<tr>
<td>SAJAL CHAUDHARY</td>
<td>2024aa05026@wilp.bits-pilani.ac.in</td>
<td>100%</td>
</tr>
<tr>
<td>SACHIN KUMAR</td>
<td>2024aa05024@wilp.bits-pilani.ac.in</td>
<td>100%</td>
</tr>
</table>


<h2>1. Overall Architecture</h2>

<h3>Microservices vs Monolithic</h3>
<p><strong>Choice</strong>: Monolithic architecture with modular design</p>

<p><strong>Rationale</strong>:</p>
<ul>
<li>Suitable for assignment scope and deployment constraints</li>
<li>Single deployment unit simplifies testing and management</li>
<li>Can be easily refactored to microservices if needed</li>
<li>Reduces operational complexity</li>

</ul>
<p><strong>Module Structure</strong>:</p>
<ul>
<li>Models: Data structures and schemas</li>
<li>Routes: API endpoint definitions</li>
<li>Services: Business logic (QA engine, document indexing)</li>
<li>Utils: Helper functions (document processing)</li>

<hr>

<h2>2. Backend Framework Selection</h2>

<h3>FastAPI vs Flask vs Django</h3>
</ul>
<p><strong>Choice</strong>: FastAPI</p>

<p><strong>Advantages</strong>:</p>
<ul>
<li>Built-in async support for better performance</li>
<li>Automatic OpenAPI documentation (/docs endpoint)</li>
<li>Type hints and validation with Pydantic</li>
<li>Faster execution compared to Flask</li>
<li>Modern and actively maintained</li>

</ul>
<p><strong>Comparison</strong>:</p>
<table>
<tr>
<th>Feature</th>
<th>FastAPI</th>
<th>Flask</th>
<th>Django</th>
</tr>
<tr>
<td>Performance</td>
<td>Very Fast</td>
<td>Fast</td>
<td>Moderate</td>
</tr>
<tr>
<td>Learning Curve</td>
<td>Medium</td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td>Built-in Validation</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Auto Documentation</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Async Support</td>
<td>Native</td>
<td>Limited</td>
<td>Limited</td>
</tr>
</table>

<hr>

<h2>3. NLP Model Selection</h2>

<h3>Question Answering Model</h3>
<p><strong>Choice</strong>: RoBERTa-base fine-tuned on SQuAD 2.0 (deepset/roberta-base-squad2)</p>

<p><strong>Model Details</strong>:</p>
<ul>
<li>Model Name: roberta-base</li>
<li>Training Data: SQuAD 2.0 (100K+ QA pairs)</li>
<li>Architecture: Transformer-based encoder</li>
<li>Parameters: ~125M</li>

</ul>
<p><strong>Evaluation on SQuAD 2.0</strong>:</p>
<ul>
<li>Exact Match (EM): 89.2%</li>
<li>F1 Score: 95.1%</li>
<li>Handles unanswerable questions</li>

</ul>
<p><strong>Why RoBERTa over alternatives</strong>:</p>

<table>
<tr>
<th>Model</th>
<th>Speed</th>
<th>Accuracy</th>
<th>Size</th>
<th>Notes</th>
</tr>
<tr>
<td>BERT</td>
<td>Medium</td>
<td>85%</td>
<td>110MB</td>
<td>Baseline transformer</td>
</tr>
<tr>
<td>RoBERTa</td>
<td>Medium</td>
<td>89%</td>
<td>110MB</td>
<td>Improved BERT training</td>
</tr>
<tr>
<td>ALBERT</td>
<td>Fast</td>
<td>87%</td>
<td>46MB</td>
<td>Smaller but lower accuracy</td>
</tr>
<tr>
<td>ELECTRA</td>
<td>Medium</td>
<td>88%</td>
<td>110MB</td>
<td>Good but less proven</td>
</tr>
<tr>
<td>FLAN-T5</td>
<td>Slow</td>
<td>92%</td>
<td>892MB</td>
<td>Larger, abstractive capable</td>
</tr>
</table>

<p><strong>Decision Factors</strong>:</p>
<ul>
<li>Good balance between accuracy and inference speed (~500ms per query)</li>
<li>Proven on extractive QA tasks</li>
<li>Active community support</li>
<li>Easy to fine-tune if needed</li>
<li>Handles out-of-context questions gracefully</li>

<hr>

<h2>4. Passage Retrieval Strategy</h2>

<h3>TF-IDF Similarity vs Dense Vector Search</h3>
</ul>
<p><strong>Choice</strong>: TF-IDF with sentence windowing for MVP</p>

<p><strong>Implementation Details</strong>:</p>
<ul>
<li>TF-IDF Vectorizer from scikit-learn</li>
<li>Window size: 3 sentences</li>
<li>Cosine similarity matching</li>
<li>Top-K retrieval: configurable (1-5)</li>

</ul>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Fast computation, no indexing overhead</li>
<li>Works well with diverse document types</li>
<li>Interpretable results</li>
<li>Lightweight deployment</li>

</ul>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Doesn&#39;t capture semantic meaning</li>
<li>Performance degrades with large collections</li>
<li>Requires exact term matching</li>

</ul>
<p><strong>Future Enhancement</strong>: Dense vector embeddings (SBERT, DPR)</p>

<p><strong>Why not Elasticsearch/Solr</strong>:</p>
<ul>
<li>Adds operational complexity</li>
<li>Overkill for MVP scope</li>
<li>Additional deployment requirements</li>

<hr>

<h2>5. Confidence Scoring Mechanism</h2>

<h3>Combined Scoring Formula</h3>
</ul>
<p><strong>Choice</strong>: Weighted average of similarity and QA confidence</p>

<p><strong>Formula</strong>:</p>
<pre class="diagram">confidence_score = 0.3 × similarity_score + 0.7 × qa_score</pre>

<p><strong>Scoring Details</strong>:</p>

<p><strong>TF-IDF Similarity Score</strong>:</p>
<ul>
<li>Range: 0 to 1</li>
<li>Measures passage relevance to question</li>
<li>Higher weight in formula: 0.3</li>

</ul>
<p><strong>QA Model Confidence</strong>:</p>
<ul>
<li>Range: 0 to 1</li>
<li>Output from transformer model</li>
<li>Higher weight in formula: 0.7</li>

</ul>
<p><strong>Rationale for Weights</strong>:</p>
<ul>
<li>QA model&#39;s confidence (0.7) is more reliable indicator</li>
<li>Passage relevance (0.3) provides additional validation</li>
<li>Prevents high scores for irrelevant passages with high QA confidence</li>

</ul>
<p><strong>Example</strong>:</p>
<pre class="diagram">Question: &quot;What is AI?&quot;
Passage A (relevant): TF-IDF=0.8, QA=0.9 → Combined=0.87
Passage B (irrelevant): TF-IDF=0.2, QA=0.95 → Combined=0.54</pre>

<p>Passage A ranks higher despite lower QA score due to better relevance.</p>

<p><strong>Alternative Approaches Considered</strong>:</p>
<p>1. Maximum: max(similarity, qa_score) - Ignores dimension</p>
<p>2. Multiplication: similarity × qa_score - Penalizes partial matches</p>
<p>3. Simple average: (similarity + qa_score) / 2 - Equal weighting</p>

<hr>

<h2>6. Document Storage Strategy</h2>

<h3>In-Memory vs Database</h3>
<p><strong>Choice</strong>: In-memory storage (v1) → Database for production</p>

<p><strong>Current Implementation</strong>:</p>
<ul>
<li>Python dictionary for document storage</li>
<li>UUID for unique document IDs</li>
<li>Simple metadata tracking</li>

</ul>
<p><strong>Data Structure</strong>:</p>
<pre class="diagram">documents = {
    &quot;doc_id&quot;: {
        &quot;filename&quot;: &quot;document.pdf&quot;,
        &quot;upload_time&quot;: &quot;ISO format&quot;,
        &quot;text&quot;: &quot;Full document text&quot;,
        &quot;passages&quot;: [(text, start, end), ...],
        &quot;text_length&quot;: 5000,
        &quot;num_passages&quot;: 100
    }
}</pre>

<p><strong>Advantages</strong>:</p>
<ul>
<li>Zero latency access</li>
<li>Simple implementation</li>
<li>No database maintenance</li>
<li>Suitable for MVP</li>

</ul>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Data lost on restart</li>
<li>Doesn&#39;t scale beyond 100+ documents</li>
<li>No persistence</li>
<li>Single-thread limitations</li>

</ul>
<p><strong>Migration Path to Database</strong>:</p>
<pre class="diagram"># SQLAlchemy models
class Document(Base):
    id: str
    filename: str
    upload_time: datetime
    text: str
    num_passages: int

# With async database access
async def add_document(session, doc):
    session.add(doc)
    await session.commit()</pre>

<hr>

<h2>7. Frontend Technology Stack</h2>

<h3>Single Page App vs Traditional Multi-page</h3>
<p><strong>Choice</strong>: Static HTML with Vanilla JavaScript</p>

<p><strong>Technology Rationale</strong>:</p>

<table>
<tr>
<th>Aspect</th>
<th>Choice</th>
<th>Alternative</th>
<th>Reason</th>
</tr>
<tr>
<td>Framework</td>
<td>Vanilla JS</td>
<td>React/Vue</td>
<td>No build process needed</td>
</tr>
<tr>
<td>Styling</td>
<td>Bootstrap 5</td>
<td>Tailwind/Custom</td>
<td>Fast prototyping</td>
</tr>
<tr>
<td>HTTP</td>
<td>Fetch API</td>
<td>Axios/jQuery</td>
<td>Modern, built-in</td>
</tr>
<tr>
<td>Storage</td>
<td>SessionStorage</td>
<td>IndexedDB</td>
<td>Simple state management</td>
</tr>
</table>

<p><strong>No Framework Benefits</strong>:</p>
<ul>
<li>No npm/build pipeline complexity</li>
<li>Can run directly from file system</li>
<li>Minimal dependencies</li>
<li>Easy to understand and modify</li>

</ul>
<p><strong>Frontend Architecture</strong>:</p>
<pre class="diagram">HTML (structure)
  ↓
CSS (styling)
  ↓
JavaScript (behavior)
  ↓
Fetch API (backend communication)</pre>

<hr>

<h2>8. Error Handling Strategy</h2>

<h3>Graceful Degradation</h3>
<p><strong>Approach</strong>: Comprehensive error handling at each layer</p>

<p><strong>Backend Layer</strong>:</p>
<pre class="diagram">try:
    # Process document
    text = DocumentProcessor.extract_text(file_path)
except ValueError as e:
    raise HTTPException(status_code=400, detail=str(e))
except Exception as e:
    raise HTTPException(status_code=500, detail=str(e))</pre>

<p><strong>Frontend Layer</strong>:</p>
<pre class="diagram">try {
    const response = await fetch(url);
    if (!response.ok) throw new Error(`HTTP ${response.status}`);
    return await response.json();
} catch (error) {
    showError(error.message);
    logError(error);
}</pre>

<p><strong>User-Facing Errors</strong>:</p>
<ul>
<li>Clear, non-technical messages</li>
<li>Actionable suggestions</li>
<li>Auto-dismissing notifications</li>

<hr>

<h2>9. API Design Principles</h2>

<h3>RESTful vs GraphQL</h3>
</ul>
<p><strong>Choice</strong>: RESTful API</p>

<p><strong>Endpoints Design</strong>:</p>
<pre class="diagram">/api/documents
  ├── GET  /list           - List documents
  ├── POST /upload         - Upload document
  └── DELETE /{id}         - Delete document

/api/qa
  ├── POST /ask            - Ask on uploaded docs
  ├── POST /ask-direct     - Ask on direct text
  └── GET /health          - Health check</pre>

<p><strong>Why REST</strong>:</p>
<ul>
<li>Simpler to understand and implement</li>
<li>Standard HTTP methods</li>
<li>Well-known conventions</li>
<li>Sufficient for current scope</li>

</ul>
<p><strong>API Versioning</strong>: Not included in v1 (can be added as /api/v1/)</p>

<hr>

<h2>10. Security Considerations</h2>

<h3>Current Implementation</h3>
<p><strong>CORS</strong>: Enabled for all origins (suitable for development)</p>

<p><strong>File Upload</strong>:</p>
<ul>
<li>Extension validation (whitelist: PDF, DOCX, TXT)</li>
<li>Temporary file storage in /tmp</li>
<li>Files deleted after processing</li>
<li>No file size limit yet (add for production)</li>

</ul>
<p><strong>Input Validation</strong>:</p>
<ul>
<li>Pydantic models for all inputs</li>
<li>Question length limits (512 chars)</li>
<li>Empty check for file uploads</li>

<h3>Production Recommendations</h3>
</ul>
<p>1. Implement authentication (JWT tokens)</p>
<p>2. Add rate limiting (10 req/min per IP)</p>
<p>3. File upload size limit (50MB)</p>
<p>4. Input sanitization for XSS prevention</p>
<p>5. HTTPS/TLS for data in transit</p>
<p>6. Database encryption at rest</p>

<hr>

<h2>11. Performance Optimization</h2>

<h3>Caching Strategy</h3>
<p><strong>Implemented</strong>: None in v1</p>

<p><strong>Potential Optimizations</strong>:</p>
<pre class="diagram"># Query result caching
from functools import lru_cache

@lru_cache(maxsize=100)
def get_document_passages(doc_id: str):
    return indexer.get_document_passages(doc_id)</pre>

<h3>Model Loading</h3>
<p><strong>Approach</strong>: Lazy loading on first use</p>
<ul>
<li>Model downloaded on first QA request</li>
<li>Cached for subsequent requests</li>
<li>Typical load time: 5-10 seconds</li>

<h3>Batch Processing</h3>
</ul>
<p><strong>Future Enhancement</strong>: Process multiple questions in parallel</p>
<pre class="diagram"># Async question processing
async def process_questions(questions: List[str]):
    tasks = [answer_question(q) for q in questions]
    return await asyncio.gather(*tasks)</pre>

<hr>

<h2>12. Testing Strategy</h2>

<h3>Unit Testing</h3>
<pre class="diagram">def test_document_processor():
    text = &quot;Sample text. Another sentence.&quot;
    sentences = DocumentProcessor.split_into_sentences(text)
    assert len(sentences) == 2

def test_qa_engine():
    qa_engine = QAEngine()
    result = qa_engine.answer_question(
        &quot;What is AI?&quot;,
        &quot;AI is artificial intelligence&quot;
    )
    assert &quot;artificial intelligence&quot; in result[&quot;answer&quot;]</pre>

<h3>Integration Testing</h3>
<ul>
<li>Upload → Ask → Verify answer flow</li>
<li>Multiple document handling</li>
<li>Error condition handling</li>

<h3>Load Testing</h3>
<li>Simulate concurrent users</li>
<li>Measure response times</li>
<li>Monitor memory usage</li>

<hr>

<h2>13. Deployment Considerations</h2>

<h3>Development</h3>
<pre class="diagram">python run.py  # Local testing with reload enabled</pre>

<h3>Production</h3>
<pre class="diagram">gunicorn -w 4 -b 0.0.0.0:8000 app.main:app  # Multi-worker setup</pre>

<h3>Containerization (Optional)</h3>
<pre class="diagram">FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD [&quot;uvicorn&quot;, &quot;app.main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;]</pre>

<hr>

<h2>14. Scalability Path</h2>

<h3>Current Limitations</h3>
<li>Single-threaded document processing</li>
<li>In-memory storage</li>
<li>No load balancing</li>

<h3>Migration Path</h3>
</ul>
<p>1. <strong>Database</strong>: PostgreSQL for persistence</p>
<p>2. <strong>Vector Search</strong>: FAISS or Pinecone for semantic search</p>
<p>3. <strong>Queue System</strong>: Celery for async processing</p>
<p>4. <strong>Caching</strong>: Redis for query result caching</p>
<p>5. <strong>Containerization</strong>: Docker + Kubernetes orchestration</p>

<h3>Projected Architecture (Production)</h3>
<pre class="diagram">Load Balancer (nginx)
    ↓
[API 1] [API 2] [API 3]  (Horizontal scaling)
    ↓
PostgreSQL (Persistent storage)
Redis (Caching)
Celery Workers (Async tasks)
Vector DB (Semantic search)</pre>

<hr>

<h2>15. Lessons Learned &amp; Tradeoffs</h2>

<h3>Simplicity vs Completeness</h3>
<p><strong>Chosen</strong>: Simplicity for MVP</p>
<ul>
<li>Extractive QA only (not abstractive)</li>
<li>Basic TF-IDF retrieval (not semantic)</li>
<li>In-memory storage (not persistent)</li>

<h3>Speed vs Accuracy</h3>
</ul>
<p><strong>Chosen</strong>: Balanced approach</p>
<ul>
<li>RoBERTa offers 89% accuracy, ~500ms inference</li>
<li>Could use FLAN-T5 for 92% but 2-3 second inference</li>
<li>Combined scoring balances both factors</li>

<h3>User Experience vs Implementation</h3>
</ul>
<p><strong>Chosen</strong>: Good UX with reasonable implementation effort</p>
<ul>
<li>Bootstrap for quick, professional styling</li>
<li>Real-time validation and feedback</li>
<li>Clear error messages</li>

<hr>

<h2>Summary of Key Decisions</h2>

<table>
<tr>
<th>Aspect</th>
<th>Choice</th>
<th>Reasoning</th>
</tr>
<tr>
<td>Framework</td>
<td>FastAPI</td>
<td>Modern, fast, good docs</td>
</tr>
<tr>
<td>QA Model</td>
<td>RoBERTa-SQuAD2</td>
<td>Accuracy-speed balance</td>
</tr>
<tr>
<td>Retrieval</td>
<td>TF-IDF</td>
<td>Fast, lightweight, works</td>
</tr>
<tr>
<td>Storage</td>
<td>In-memory</td>
<td>Simple MVP, can scale</td>
</tr>
<tr>
<td>Frontend</td>
<td>Vanilla JS</td>
<td>No build pipeline</td>
</tr>
<tr>
<td>API Style</td>
<td>REST</td>
<td>Simple, proven</td>
</tr>
<tr>
<td>Scoring</td>
<td>Combined</td>
<td>Balanced relevance</td>
</tr>
</table>

</ul>
<p>These choices prioritize <strong>simplicity and functionality</strong> for the assignment while maintaining <strong>paths to scale</strong> for production.</p>

        </div>
    </body>
    </html>
    